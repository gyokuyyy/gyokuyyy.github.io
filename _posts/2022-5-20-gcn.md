---
layout: page
title:  "Graph Convolutional Network(GCN)概论"
subtitle: "Graph Convolutional Network"
date:   2022-05-20 21:21:21 +0530
categories: misc
---

# 图卷积概论基础

## 卷积定义与卷积神经网络CNN

**卷积定义**
$$
\large {
(f*g)(n)=\int f(z)g(n-z)dz }
$$
**CNN中的卷积定义**

采用了离散卷积的形式，即输入X为一个矩阵，卷积核为另一个矩阵，在进行卷积时，结果矩阵的每个点就是当n取不同下标【i，j】时，两个矩阵重合部分的对应乘积再累加。当【i，j】变化时，就相当于上述公式中的n在变化。卷积结果相当于关于【i，j】或n的一个函数的输出值。给定一个[i,j]或n，就得到一个输出矩阵[i,j]位置的值。

## 图卷积发展

### 早期的时域图卷积

早期利用以下公式进行局部特征提取:
$$
\large {
h_v^{l+1}=\sum _{i \in N(v)} w_i h_i^l \\
v:结点，l:当前网络层，i\in N(v) v的邻居节点集合，w_i为与v相连的边权值，h_i^l:结点i在l层的值的
}
$$


在l层的输出结果中，节点v的值（类比于CNN卷积后输出的feature map中的一个点的值）由l层的输入图数据中与节点v相邻的其他节点（也可以包括v自己）的取值进行加权累加。

缺点：没有了卷积核概念，没有卷积核作为共享参数，只是简单进行了特征的汇聚；另外，当图的规模很大时，计算量也是非常大的。

### 频域图卷积预备知识

#### 傅里叶变换

将一个时域的函数（广义的向量）转到频域空间，就要找到合适的频域空间的基向量。

傅里叶级数（针对周期性的函数），傅立叶变换（处理非周期函数）。

傅里叶变换：
$$
\large {
F(\omega)=\mathcal{F}[f(t)]=\int_{-\infty}^{+\infty} f(t)e^{-i\omega t}dt }
$$
频域的信号是F(w)，时域的信号是f(t)。**可以将这个积分理解为f(t)向量与$e^{-e\omega t}$向量的内积（投影）**。而$e^{-e\omega t}$就是撑起频域空间的一组基向量，而且是正交基。那么F(w)就是频域空间中这个信号的坐标了。所以同一个信号，就从时域空间投影到了频域空间。

#### 拉普拉斯算子

GCN中利用拉普拉斯算子和傅里叶变换的基向量建立联系，寻找频域的$e^{-i\omega t}$基向量。

拉普拉斯算子是n维欧几里德空间中的一个二阶微分算子，定义为梯度（▽f）的散度（▽·f）。因此如果f是二阶可微的实函数，则**f的拉普拉斯算子**定义为：
$$
\large{
\Delta f=\nabla^2f=\nabla.\nabla f=\sum_{i=1}^n \frac{\partial^2f}{\partial x_i^2}
\\
\Delta为拉普拉斯算子 
}
$$

**拉普拉斯算子在离散函数上的定义**
$$
\large{
\begin{align}
&在笛卡尔坐标系下，梯度\nabla=\frac{\partial}{\partial x}\vec i+\frac{\partial}{\partial y}\vec j+\frac{\partial}{\partial z}\vec k \\
&旋度\nabla \cdot=\frac{\partial}{\partial x}+\frac{\partial}{\partial y}+\frac{\partial}{\partial z} \\
&拉普拉斯算子\Delta f=\nabla^2f=\nabla \cdot \nabla f=div(grad \ f)，\\
&在笛卡尔坐标系下，\Delta f=\frac{\partial^2 f}{\partial x^2}+\frac{\partial^2f}{\partial y^2}+\frac{\partial^2f}{\partial z^2} ,n维形式：\Delta f=\sum_i \frac{\partial^2 f}{\partial x_i^2} \\
&在离散函数形式下，\frac{\partial f}{\partial x}=f(x+1)-f(x)， \\
&\frac{\partial^2 f}{\partial x^2}=f^{'}(x)-f^{'}(x-1)=f(x+1)+f(x-1)-2f(x) \\
&在二维情况下，\Delta f=\frac{\partial^2 f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2}\\
&=f(x+1,y)+f(x-1,y)-2f(x,y)+f(x,y+1)+f(x,y-1)-2f(x,y) \\
&=f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-\textcolor{red}{4f(x,y)} 
\end{align}
}
$$


**拉普拉斯算子在图上的应用**

 假设具有 N个节点的图 G ，此时以上定义的函数f不再是二维，而是 N 维向量：$f=(f_1,f_2,……,f_N)$ ，其中$f_i$ 为函数 f在图中节点 i处的函数值。类比于 f(x,y) 在节点 (x,y)处的值。对 i节点进行扰动，它可能变为任意一个与它相邻的节点$j \in N_i$, $N_i$表示节点 i 的一阶邻域节点。

拉普拉斯算子可以计算一个点到它所有自由度上微小扰动的增益，则通过图来表示就是任意一个节点 i变化到节点j所带来的增益，考虑图中边的权值为$w_{ij}$则有
$$
\large{
\Delta f_i=\sum _{j \in N_i}w_{ij}(f_i-f_j)}
$$
![img](C:\Users\xur20\OneDrive - nuaa.edu.cn\硕士学习\笔记\基础学习.assets\v2-6b498216b66bdc5a81d7876ca665fd32_720w.jpg)

**拉普拉斯算子的特征向量就是傅里叶变换的频域基向量**$\Large{e^{-i\omega t}}$
$$
\large{
\Delta f=\nabla^2f=\sum_{i=1}^n \frac{\partial^2f}{\partial x_i^2} \\
令f=e^{-i\omega t},则\\ 
\Delta e^{-i\omega t}=-\omega^2 e^{-i\omega t}  \\
\Delta对于A，e^{-i\omega t}对应\vec{x},-\omega^2 对应\lambda,e^{-i\omega t} 为\Delta特征向量
}
$$



#### 拉普拉斯矩阵

**拉普拉斯矩阵**：

定义：若G是简单图，G有n个顶点，A是邻接矩阵，D是度数矩阵，则拉普拉斯矩阵为
$$
\large{
L=D-A \\
f(x)=\left\{
\begin{aligned}
&deg(v_i)  \quad &if \ \ i=j \\
&-1 \quad &if \ \ i \neq j \ and \ v_i \ is \ adjacent\  to \ v_j \\
&0 \quad &otjerwise
\end{aligned}
\right.
}
$$




若对新的laplacian operator中添加了边上的权值，则可以简化定义为
$$
\large {
\begin{align}
&\Delta f_i = \sum_{j \in N_i} w_{ij}(f_i-f_j) \qquad 对于结点i 的\Delta值，N_i:邻居，w_{ij}为权值
\\
&=\sum_{j \in N} w_{ij}(f_i-f_j)  \qquad    简写
\\
&=\sum_{j \in N} w_{ij}f_i-\sum_{j \in N} w_{ij}f_j \qquad 令\sum_{i \in N} w_{ij}=d_i \ ,即结点i的加权度
\\
&=d_if_i-\sum_{j \in N} w_{ij}f_j
\\
&=d_if_i-w_{i:}^Tf \qquad 后一项为向量內积
\\ \\
&对于所有结点的拉普拉斯算子，\\
&\textcolor{red}{\Delta f}=\begin{bmatrix} \Delta f_1 \\ \Delta f_2 \\ …… \\ \Delta f_N \\ \end{bmatrix}
=\begin{bmatrix} d_1f_1-w_{1:}^Tf \\ d_2f_2-w_{2:}^Tf \\ …… \\ d_Nf_N-w_{N:}^Tf\end{bmatrix}
=\begin{bmatrix} d_1f_1 \\ d_2f_2 \\ …… \\ d_Nf_N\end{bmatrix}-\begin{bmatrix} w_{1:}^Tf \\ w_{2:}^Tf \\ …… \\ w_{N:}^Tf\end{bmatrix}
\\ \\
&=\begin{bmatrix} d_1 & 0 & 0 &……& 0 \\ 0 & d_2 & 0 & …… & 0 \\  \vdots \\ \vdots \\ 0 & 0 & 0 & …… & d_N   \end{bmatrix} \begin{bmatrix} f_1 \\ f_2 \\ \vdots \\ \vdots \\ f_N \end{bmatrix}-\begin{bmatrix} w_{1:} \\ w_{2:} \\ \vdots \\ \vdots \\ w_{N:} \end{bmatrix} f  
\\  &其中f为n \times 向量，值为图中的每个结点值(f_1,……，f_N),w_{1:}为行向量  \\ \\
&=diag(d_i)f-wf  \\\
&=(D-W)f \\
&=\textcolor{red}{Lf}

\end{align}
}
$$


有拉普拉斯算子推导可以得到 $\Delta f=(D-W)f$，而拉普拉斯矩阵定义为$L=D-A$，A是邻接矩阵，而W是带权重的邻接矩阵，因此$\Delta f=(D-W)f=Lf$

综上，**GCN中对拉普拉斯矩阵的使用，就是对拉普拉斯算子的使用，而且拉普拉斯算子的特征值可以看作是频域空间的基向量**。



#### 图的傅里叶变换



对拉普拉斯矩阵L进行特征分解，得到特征向量，并由特征向量构成矩阵U，即 $L=U\Lambda U^T$。U的每一列都是L的特征向量，就是频域的基向量$e^{-i\omega t}$。

根据前述对傅里叶变换的理解，那个积分表达式$\large {
F(\omega)=\mathcal{F}[f(t)]=\int_{-\infty}^{+\infty} f(t)e^{-i\omega t}dt }$ 可以理解为时域的函数f(t)向量与$e^{-i\omega t}$向量的内积（投影）。

**傅里叶变换**可以写为
$$
\large{
\begin{align}
& L=U \Lambda U^T \quad U的每一列都是L的特征向量，即频域基向量e^{-iwt} \\
&\LARGE \textbf{傅里叶变换}： \\
&\hat{f}=U^Tf \\
&\hat{f}为频域空间中的新向量[\hat{f}(1),……，\hat f(N)]，\\ 
&U为频域空间基向量，f为时域空间向量[f(1)……f(N)]  	\\
&\LARGE \textbf{傅里叶逆变换}： \\
&f=U\hat f
&

\end{align}
}
卷积定理
$$


**卷积定理**：建立了时域卷积与频域上计算的对应关系
$$
\large{
\begin{align} 
&由于时域的上的卷积不好计算，先利用傅里叶变换转换到频域上内的內积，再傅里叶逆变换回来 \\
&f*g=F^{-1}[F(f) \cdot F(g)]
\end{align}
}
$$
**卷积定理证明如下**：
$$
\large{
\begin{align}

&F(f_1(t)*f_2(t))  \\
=&\int_{-\infty}^{+\infty} [ \int_{-\infty}^{+\infty} f_1(\tau) f_2(t-\tau) d\tau]e^{-i\omega t} dt \\
=&\int_{-\infty}^{+\infty} f_1(\tau)  [ \int_{-\infty}^{+\infty} f_2(t-\tau) e^{-i\omega t} dt] d\tau \\
=&\int_{-\infty}^{+\infty} f_1(\tau)  [ e^{-i\omega \tau} \int_{-\infty}^{+\infty} f_2(t-\tau) e^{-i\omega t-\tau} d(t-\tau)] d\tau \\
=&\int_{-\infty}^{+\infty} f_1(\tau) F_2(\omega) d\tau \\
=&F_1(\omega) F_2(\omega)  \\

所以&f_1(t)*f_2(t))=F^{-1}(F_1(\omega) F_2(\omega))

\end{align}
}
$$


**图的卷积**可表示为
$$
\large{
\begin{align}
&傅里叶变换：\hat f=U^Tf \ ,逆变换：f=U \hat f   \\
&f*g=F^{-1}(F(f) \cdot F(g)) \\
&=U \cdot [U^Tf \odot U^Tg ] \quad 逐元素计算，定义g_\theta=U^Tg作为对角元素构成的矩阵 ，\ 即\textcolor{red}{卷积核}\\
&=Ug_\theta \cdot U^Tf \\
& \textcolor{red}{Note:} g_\theta 为N \times N 矩阵，实际上的g_\theta是U^Tg 的N \times 1向量的每个值作为对角元素构成的矩阵，\\
&g_\theta =\begin{bmatrix}\theta_1 & 0 & …… &0 \\ \vdots \\ \vdots \\ 0 & 0 & …… & \theta_N\end{bmatrix}
\end{align}

}
$$

### 第一代GCN


$$
\large{
\begin{align}
& h_{ij}^{l+1}=\sigma(Ug_\theta U^T h_{:j}^l)= \sigma(U\begin{bmatrix} \theta_1 0& …… & 0 \\ \vdots \\ \vdots \\ 0&……&\theta_N \end{bmatrix} U^T h_{:j}^l)\\ 
&l+1层中结点i的隐向量中来自结点j的贡献,h_{:j}^l为结点j的隐向量\\
&h_i^{l+1}是h_{:j}^l左乘Ug_\theta U^T 之后，累加得到的，即l+1层的结点i的隐向量是l层所结点 \\
&即(f*g)_G=\sigma(Ug_\theta U^Tx),\quad x为所有结点的输入,x \in R^{N \times C}

\end{align}
}
$$

缺点：需要从拉普拉斯矩阵的特征分解来获得U，当N很大时，拉普拉斯矩阵L的维度N*N，特征分解的计算量就很大。同时，矩阵相乘的复杂度也很高O（n^2），最后，没有使用归一化的拉普拉斯矩阵，也就会存在度很大的节点影响过大。

### 第二代GCN

在第一代GCN中$g_\theta$是$U^Tg$的向量构成的对角矩阵，$\large{g_\theta=\begin{bmatrix} \theta_1 & \\ & \theta_2 & \\ & & \ddots \theta_N \end{bmatrix}}$有N个参数。

在第二代GCN中
$$
\large{
\begin{align}
令&g_{\theta^,} \approx \sum_{k=0}^K \theta_k^{'}\Lambda^k 来代替g_\theta,\\
&\theta_k^{'}为k个待学习参数，\Lambda为L的特征向量构成，参数量从N下降为与结点数无关的K个  \\
&g_{\theta^{'}}*x=U\sum_{k=0}^K \theta_k^{'}\Lambda^k U^T x  \quad （U与K是无关的） \\
&=\sum_{k=0}^K \theta_k^{'} (U\Lambda^k U^T )x \\
&=\sum_{k=0}^K \theta_k^{'} (U\Lambda U^T )^k x  \quad (L=U\Lambda U^T 特征分解) \\
&=\sum_{k=0}^K \theta_k^{'} L^k x \\
&卷积公式中只有L，不用再对L求特征分解U  \\

&\textcolor{BLUE}{NOTE}:\\
&(U\Lambda U^T )^k=(U\Lambda U^T)(U\Lambda U^T)……(U\Lambda U^T) \\
&=U\Lambda I U\Lambda I……I\Lambda U^T \\
&=U \Lambda^K U^T
\end{align}
}
$$
缺点：虽然不用特征分解了，但是矩阵相乘仍然计算量很大，计算L的k次方，复杂度$O(kn^2)$，且没有使用归一化的拉普拉斯矩阵。

要注意的是，超参数K为卷积核的receptive field感受野，也就是节点i周围的K-跳的节点的h会影响到节点i。

### 第三代GCN



第三代GCN利用切比雪夫展开式进一步对待学习的参数$g_\theta$进行了近似。
$$
\large{
\begin{align}
&令g_{\theta^{'}}(\Lambda)=\sum_{k=0}^K\theta_k^{'}T_k(\overline{\Lambda}),其中\overline{\Lambda}=\frac{2}{\lambda_{max}}\Lambda-I_N,\lambda_{max}为\Lambda中最大特征值，I_N：单位矩阵	\\
&利用T_k(\overline{\Lambda})代替\Lambda^k,切比雪夫展开式(递归): T_k(x)=2xT_{k-1}-T_{k-2}(x),T_0(x)=1,T_1(x)=x \\
&\Large \textcolor{red}{卷积公式}： \\
&g_{\theta^{'}}*x=U\sum_{k=0}^K \theta_k^{'} T_k(\overline{\Lambda}) U^T x \\
&=\sum_{k=0}^K \theta_k^{'} UT_k(\overline{\Lambda}) U^T x \quad (因为T_k(\overline{\Lambda})是关于\Lambda的多项式，所以U与U^T可以进入括号内) \\
&=\sum_{k=0}^K \theta_k^{'} T_k(U \overline{\Lambda}  U^T )x \\
&=\sum_{k=0}^K \theta_k^{'} T_k(\overline{L} )x \\
&其中\overline L=U \overline{\Lambda} U^T =\frac{2}{\lambda_{max}}U \Lambda U^T-U I_N U^T=\frac{2}{\lambda_{max}}L - I_N
\end{align}
}
$$
对于切比雪夫展开式K阶展开的进一步简化：
$$
\large{
\begin{align}
&1. 令k=1,则切比雪夫展开式只有k=0与k=1两项 											 \\ &\\

&2.令\lambda_{max}=2，则\overline{L}=\frac{2}{\lambda_{max}}L-I_N 					\\
&g_{\theta^{'}}*x=\sum_{k=0}^1 \theta_k^{'} T_k(\overline{L} )x =\theta_0^{'} T_0(\overline{L} )x+\theta_1^{'} T_1(\overline{L} )x  \quad (由于T_0(x)=1,T_1(x)=x)\\
&=\theta_0^{'}x+\theta_1^{'}\overline{L}x =\theta_0^{'}x+\theta_1^{'}(L-I_N)x 	\\	& \\

&3.令\hat{L}=D^{-\frac 1 2} L D^{-\frac 1 2},(D为度矩阵，\textcolor{red}{为什么是D没搞懂})，即使用了归一化的拉普拉斯矩阵\hat L来代替L \\
&利用\hat L=D^{-\frac 1 2}L进行归一化无法得到对称矩阵，而L=D-W是对矩阵，					\\
&用\hat L=D^{-\frac 1 2}LD^{-\frac 1 2}进行归一化得到的依旧是对称矩阵，证明如下 \\
&\hat L \cdot \hat L^T=D^{-\frac 1 2}LD^{-\frac 1 2} ( D^{-\frac 1 2}LD^{-\frac 1 2})^T=D^{-\frac 1 2} L D^{-\frac 1 2} (D^{-\frac 1 2})^T L^T (D^{-\frac 1 2})^T \\
&=D^{-\frac 1 2}L L^T (D^{-\frac 1 2})^T=D^{-\frac 1 2}(D^{-\frac 1 2})^T=I \\
&所以\hat L是对称矩阵 。\textcolor{red}{（此处存在疑问）}   							\\ &	\\
&所以\hat L==D^{-\frac 1 2}LD^{-\frac 1 2} =D^{-\frac 1 2} (D-W) D^{-\frac 1 2}=I_N- D^{-\frac 1 2} W D^{-\frac 1 2} \\
&g_{\theta^{'}}*x=\theta_0^{'}x+\theta_1^{'}(L-I_N)x \quad  (用\overline L代替L)   \\
&=\theta_0^{'}x+\theta_1^{'}(I_N- D^{-\frac 1 2} W D^{-\frac 1 2}-I_N)x  \\
&=\theta_0^{'}x-\theta_1^{'} D^{-\frac 1 2} W D^{-\frac 1 2}x  \\ & \\

&4.\theta_0^{'}=-\theta_1^{'}=\theta ,则 \\
&g_{\theta^{'}}*x=\theta x + \theta D^{-\frac 1 2} W D^{-\frac 1 2}x  \\
&=\theta (I_N+  D^{-\frac 1 2} W D^{-\frac 1 2}) x  \\
&令\widetilde{W}=W+I_N,则I_N+  D^{-\frac 1 2} W D^{-\frac 1 2} \rightarrow  \widetilde {D}^{-\frac 1 2}  \widetilde {W}  \widetilde{D}^{-\frac 1 2} \textcolor{red}{(此处的 \widetilde{D}^{-\frac 1 2} 未理解)}\\
&故g_{\theta^{'}}*x=\theta (\widetilde {D}^{-\frac 1 2}  \widetilde {W}  \widetilde{D}^{-\frac 1 2}) x

\end{align}
}
$$

